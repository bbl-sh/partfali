# Deep Learning - Course Layout

## Week 1

- History of deep learning.
- Deep learning success stories.
- McCulloch Pitts neuron, thresholding logic, perceptrons.
- Perceptron learning algorithm.

## Week 2

- Multilayer perceptrons.
- Representation power.
- Sigmoid neurons.
- Gradient descent.
- Feedforward networks.

## Week 3

- Feedforward neural networks.
- Backpropagation.

## Week 4

- Gradient descent variants: momentum, Nesterov, stochastic, AdaGrad, RMSProp, Adam.
- Eigenvalues and eigenvectors.
- Eigenvalue decomposition, basis.

## Week 5

- Principal component analysis.
- Singular value decomposition.

## Week 6

- Autoencoders and relation to PCA.
- Regularization in autoencoders.
- Denoising, sparse, and contractive autoencoders.

## Week 7

- Regularization: bias-variance tradeoff.
- L2 regularization, early stopping.
- Data augmentation.
- Parameter sharing and tying.
- Injecting noise at input.
- Ensemble methods, dropout.

## Week 8

- Greedy layerwise pre-training.
- Better activation functions.
- Better weight initialization.
- Batch normalization.

## Week 9

- Learning vectorial representations of words.

## Week 10

- Convolutional neural networks.
- LeNet, AlexNet, ZF-Net, VGGNet, GoogLeNet, ResNet.
- Visualizing CNNs, guided backprop, deep dream, deep art, fooling CNNs.

## Week 11

- Recurrent neural networks.
- BPTT, vanishing/exploding gradients, truncated BPTT.
- GRU, LSTMs.

## Week 12

- Encoder-decoder models.
- Attention mechanism.
- Attention over images.
