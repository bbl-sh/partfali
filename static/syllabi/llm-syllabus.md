# LLM Syllabus - Course Layout

## Week 1

- Introduction to NLP.
- Word and sentence level tasks.
- N-gram language models.

## Week 2

- Introduction to deep learning.
- Shallow and deep neural networks.
- Representation learning.

## Week 3

- Word representations: Word2Vec, GloVe, fastText.
- Multilingual representations with emphasis on Indian languages.

## Week 4

- Recurrent neural networks.
- RNN LMs, GRUs, LSTMs, Bi-LSTMs.
- LSTMs for sequence labeling and sequence-to-sequence.

## Week 5

- Attention mechanism.
- Sequence-to-sequence with attention.
- Transformers: attention is all you need.

## Week 6

- Self-supervised learning and pretraining.
- Designing SSL objectives.
- ELMO, BERT, GPT, T5, BART.

## Week 7

- Applications: question answering, dialog modeling, text summarization.
- Multilingual extensions for Indian languages.

## Week 8

- Instruction fine-tuning.
- FLAN-T5.
- Reinforcement learning through human feedback (RLHF).

## Week 9

- In-context learning and chain-of-thought prompting.
- Scaling laws.
- Large language model architectural differences.

## Week 10

- Parameter-efficient fine-tuning (PEFT): LoRA, QLoRA.

## Week 11

- Handling long context.
- Retrieval augmented generation (RAG).

## Week 12

- Analysis and interpretability.
- Ethical considerations.
